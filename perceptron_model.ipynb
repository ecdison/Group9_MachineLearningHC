{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecdison/Group9_MachineLearningHC/blob/anna-sophia0/perceptron_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1de4fe1",
      "metadata": {
        "id": "c1de4fe1"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib as plt\n",
        "\n",
        "X_train = pd.read_csv('train_x.csv')\n",
        "y_train = pd.read_csv('train_y.csv')\n",
        "X_test = pd.read_csv('test_x.csv')\n",
        "y_test = pd.read_csv('test_y.csv')\n",
        "\n",
        "# Make output stable across all models\n",
        "np.random.seed(42)\n",
        "\n",
        "# Only use train set. x is df without live birth. y is live birth occurence."
      ],
      "metadata": {
        "id": "EQrJVXkG6PWh"
      },
      "id": "EQrJVXkG6PWh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2c4a082",
      "metadata": {
        "id": "d2c4a082"
      },
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.activation_func = self._unit_step_func\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        \n",
        "    # Implement the fit & predict method\n",
        "    \n",
        "    #Defines training functions & label\n",
        "    def fit(self, X, y): \n",
        "        # Get the dimensions of the x array\n",
        "        # X array is an nd array of size m*n. Where m is number of samples (rows) & n is number of columns (features)\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        #init weights & we set them to 0 in the beginning\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        \n",
        "        # We have to make sure our y consists of classes 0 & 1 only using list comprehension\n",
        "        y_ = np.array([1 if i > 0 else 0 for i in y])\n",
        "        \n",
        "        # We iterate over the training samples by giving us the index & the current sample\n",
        "        # We are applying activation function to 1 sample\n",
        "        for _ in range(self.n_iters):\n",
        "\n",
        "            for idx, x_i in enumerate(X):\n",
        "\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = self.activation_func(linear_output)\n",
        "\n",
        "                # Perceptron update rule\n",
        "                update = self.lr * (y_[idx] - y_predicted)\n",
        "\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "    \n",
        "    # Define predict method\n",
        "    # We apply the linear & then activation function \n",
        "    # Linear function is w transposed by x + bias (which is the dot product of w & x)\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        \n",
        "        #Define activation function & are using activation function for multiple samples\n",
        "        y_predicted = self.activation_func(linear_output)\n",
        "        return y_predicted\n",
        "    \n",
        "    # Unit step function is the activation function \n",
        "    def _unit_step_func(self, x):\n",
        "        return np.where(x >= 0, 1, 0) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31fbec3a",
      "metadata": {
        "id": "31fbec3a"
      },
      "outputs": [],
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c16c36c",
      "metadata": {
        "id": "0c16c36c",
        "outputId": "acc101a2-872d-4e32-a2d8-92e606116f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0e88192151a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Create perceptron, fit the data & predict test label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Perceptron' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    def accuracy(y_true, y_pred):\n",
        "            accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "            return accuracy \n",
        "\n",
        "    # Split the data & create 2 blobs using make_blobs to create 2 classes & split data into training & test samples & training & test labels\n",
        "    X, y = datasets.make_blobs(n_samples=150, n_features=2, centers=2, cluster_std=1.05, random_state=2)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "    # Create perceptron, fit the data & predict test label\n",
        "    p = Perceptron(learning_rate=0.01, n_iters=1000)\n",
        "    p.fit(X_train, y_train)\n",
        "    predictions = p.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy & plot it\n",
        "    print(\"Perceptron classification accuracy\", accuracy(y_test, predictions))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], marker=\"o\", c=y_train)\n",
        "\n",
        "    x0_1 = np.amin(X_train[:, 0])\n",
        "    x0_2 = np.amax(X_train[:, 0])\n",
        "\n",
        "    x1_1 = (-p.weights[0] * x0_1 - p.bias) / p.weights[1]\n",
        "    x1_2 = (-p.weights[0] * x0_2 - p.bias) / p.weights[1]\n",
        "\n",
        "    ax.plot([x0_1, x0_2], [x1_1, x1_2], \"k\")\n",
        "\n",
        "    ymin = np.amin(X_train[:, 1])\n",
        "    ymax = np.amax(X_train[:, 1])\n",
        "    ax.set_ylim([ymin - 3, ymax + 3])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Perceptron only work for linearly seperapble functions, otherwise it isn't so great. \n",
        "# We can try different activation functions e.g. Sigmund function & apply regression "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "057cec53",
      "metadata": {
        "id": "057cec53"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}