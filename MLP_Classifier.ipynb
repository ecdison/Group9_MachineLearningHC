{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ee3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "train_x = pd.read_csv('csv_files/train_binary_x.csv')\n",
    "train_y = pd.read_csv('csv_files/train_binary_y.csv')\n",
    "test_x = pd.read_csv('csv_files/test_binary_x.csv')\n",
    "test_y = pd.read_csv('csv_files/test_binary_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36aecb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP = MLPClassifier(activation='relu', alpha=0.05, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(20, 50, 70, 100, 70, 50, 20),\n",
    "       learning_rate='adaptive', learning_rate_init=0.001, max_iter=100,\n",
    "       momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
    "       power_t=0.5, random_state=None, shuffle=True, solver='sgd',\n",
    "       tol=0.0001, validation_fraction=0.1, verbose=True,\n",
    "       warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41764e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anna-Sophia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38430970\n",
      "Iteration 2, loss = 0.24292010\n",
      "Iteration 3, loss = 0.15332812\n",
      "Iteration 4, loss = 0.13512367\n",
      "Iteration 5, loss = 0.13825633\n",
      "Iteration 6, loss = 0.13293278\n",
      "Iteration 7, loss = 0.13232534\n",
      "Iteration 8, loss = 0.14824882\n",
      "Iteration 9, loss = 0.13190785\n",
      "Iteration 10, loss = 0.13150219\n",
      "Iteration 11, loss = 0.13140983\n",
      "Iteration 12, loss = 0.13106342\n",
      "Iteration 13, loss = 0.13101424\n",
      "Iteration 14, loss = 0.13077009\n",
      "Iteration 15, loss = 0.13048002\n",
      "Iteration 16, loss = 0.13030294\n",
      "Iteration 17, loss = 0.13335225\n",
      "Iteration 18, loss = 0.12998883\n",
      "Iteration 19, loss = 0.12975679\n",
      "Iteration 20, loss = 0.13452896\n",
      "Iteration 21, loss = 0.12963489\n",
      "Iteration 22, loss = 0.12937853\n",
      "Iteration 23, loss = 0.12924755\n",
      "Iteration 24, loss = 0.12894153\n",
      "Iteration 25, loss = 0.12898397\n",
      "Iteration 26, loss = 0.12870002\n",
      "Iteration 27, loss = 0.12855000\n",
      "Iteration 28, loss = 0.12841544\n",
      "Iteration 29, loss = 0.12832526\n",
      "Iteration 30, loss = 0.12830188\n",
      "Iteration 31, loss = 0.12801257\n",
      "Iteration 32, loss = 0.12786492\n",
      "Iteration 33, loss = 0.12771712\n",
      "Iteration 34, loss = 0.12781547\n",
      "Iteration 35, loss = 0.12756548\n",
      "Iteration 36, loss = 0.12726457\n",
      "Iteration 37, loss = 0.12719728\n",
      "Iteration 38, loss = 0.12701822\n",
      "Iteration 39, loss = 0.12681687\n",
      "Iteration 40, loss = 0.12682281\n",
      "Iteration 41, loss = 0.12666762\n",
      "Iteration 42, loss = 0.12669404\n",
      "Iteration 43, loss = 0.12639070\n",
      "Iteration 44, loss = 0.12641778\n",
      "Iteration 45, loss = 0.12620686\n",
      "Iteration 46, loss = 0.12602744\n",
      "Iteration 47, loss = 0.12598284\n",
      "Iteration 48, loss = 0.12602492\n",
      "Iteration 49, loss = 0.13361087\n",
      "Iteration 50, loss = 0.12557912\n",
      "Iteration 51, loss = 0.12561490\n",
      "Iteration 52, loss = 0.12530164\n",
      "Iteration 53, loss = 0.12524801\n",
      "Iteration 54, loss = 0.12506002\n",
      "Iteration 55, loss = 0.12493210\n",
      "Iteration 56, loss = 0.12471425\n",
      "Iteration 57, loss = 0.12466959\n",
      "Iteration 58, loss = 0.12447998\n",
      "Iteration 59, loss = 0.12434969\n",
      "Iteration 60, loss = 0.12419109\n",
      "Iteration 61, loss = 0.12414348\n",
      "Iteration 62, loss = 0.12392702\n",
      "Iteration 63, loss = 0.12735363\n",
      "Iteration 64, loss = 0.12381133\n",
      "Iteration 65, loss = 0.12377290\n",
      "Iteration 66, loss = 0.12358978\n",
      "Iteration 67, loss = 0.12345955\n",
      "Iteration 68, loss = 0.12330921\n",
      "Iteration 69, loss = 0.12315684\n",
      "Iteration 70, loss = 0.12307788\n",
      "Iteration 71, loss = 0.12303219\n",
      "Iteration 72, loss = 0.12289251\n",
      "Iteration 73, loss = 0.12273826\n",
      "Iteration 74, loss = 0.12261757\n",
      "Iteration 75, loss = 0.12247714\n",
      "Iteration 76, loss = 0.12243837\n",
      "Iteration 77, loss = 0.12217975\n",
      "Iteration 78, loss = 0.12209512\n",
      "Iteration 79, loss = 0.12204480\n",
      "Iteration 80, loss = 0.12196880\n",
      "Iteration 81, loss = 0.12179639\n",
      "Iteration 82, loss = 0.12165424\n",
      "Iteration 83, loss = 0.12157260\n",
      "Iteration 84, loss = 0.12144468\n",
      "Iteration 85, loss = 0.12137596\n",
      "Iteration 86, loss = 0.12123177\n",
      "Iteration 87, loss = 0.12110212\n",
      "Iteration 88, loss = 0.12114615\n",
      "Iteration 89, loss = 0.12090584\n",
      "Iteration 90, loss = 0.12076588\n",
      "Iteration 91, loss = 0.12086758\n",
      "Iteration 92, loss = 0.12061214\n",
      "Iteration 93, loss = 0.12044751\n",
      "Iteration 94, loss = 0.12034742\n",
      "Iteration 95, loss = 0.12031449\n",
      "Iteration 96, loss = 0.12015883\n",
      "Iteration 97, loss = 0.12026298\n",
      "Iteration 98, loss = 0.11993093\n",
      "Iteration 99, loss = 0.11982381\n",
      "Iteration 100, loss = 0.11972979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anna-Sophia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.05, hidden_layer_sizes=(20, 50, 70, 100, 70, 50, 20),\n",
       "              learning_rate='adaptive', max_iter=100, solver='sgd',\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = train_y.astype('int')\n",
    "MLP.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd721c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MLP.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2639522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     27432\n",
      "           1       0.76      1.00      0.86      3773\n",
      "\n",
      "    accuracy                           0.96     31205\n",
      "   macro avg       0.88      0.98      0.92     31205\n",
      "weighted avg       0.97      0.96      0.96     31205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213e05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
=======
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecdison/Group9_MachineLearningHC/blob/anna-sophia0/MLP_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ee3bcf",
      "metadata": {
        "id": "41ee3bcf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "train_x = pd.read_csv('csv_files/train_binary_x.csv')\n",
        "train_y = pd.read_csv('csv_files/train_binary_y.csv')\n",
        "test_x = pd.read_csv('csv_files/test_binary_x.csv')\n",
        "test_y = pd.read_csv('csv_files/test_binary_y.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36aecb08",
      "metadata": {
        "id": "36aecb08"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "MLP = MLPClassifier(activation='relu', alpha=0.05, batch_size='auto', beta_1=0.9,\n",
        "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
        "       hidden_layer_sizes=(20, 50, 70, 100, 70, 50, 20),\n",
        "       learning_rate='adaptive', learning_rate_init=0.001, max_iter=100,\n",
        "       momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
        "       power_t=0.5, random_state=None, shuffle=True, solver='sgd',\n",
        "       tol=0.0001, validation_fraction=0.1, verbose=True,\n",
        "       warm_start=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41764e93",
      "metadata": {
        "id": "41764e93",
        "outputId": "7e33a58e-26a8-4898-c40d-8b6c9a16d42b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Anna-Sophia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.38430970\n",
            "Iteration 2, loss = 0.24292010\n",
            "Iteration 3, loss = 0.15332812\n",
            "Iteration 4, loss = 0.13512367\n",
            "Iteration 5, loss = 0.13825633\n",
            "Iteration 6, loss = 0.13293278\n",
            "Iteration 7, loss = 0.13232534\n",
            "Iteration 8, loss = 0.14824882\n",
            "Iteration 9, loss = 0.13190785\n",
            "Iteration 10, loss = 0.13150219\n",
            "Iteration 11, loss = 0.13140983\n",
            "Iteration 12, loss = 0.13106342\n",
            "Iteration 13, loss = 0.13101424\n",
            "Iteration 14, loss = 0.13077009\n",
            "Iteration 15, loss = 0.13048002\n",
            "Iteration 16, loss = 0.13030294\n",
            "Iteration 17, loss = 0.13335225\n",
            "Iteration 18, loss = 0.12998883\n",
            "Iteration 19, loss = 0.12975679\n",
            "Iteration 20, loss = 0.13452896\n",
            "Iteration 21, loss = 0.12963489\n",
            "Iteration 22, loss = 0.12937853\n",
            "Iteration 23, loss = 0.12924755\n",
            "Iteration 24, loss = 0.12894153\n",
            "Iteration 25, loss = 0.12898397\n",
            "Iteration 26, loss = 0.12870002\n",
            "Iteration 27, loss = 0.12855000\n",
            "Iteration 28, loss = 0.12841544\n",
            "Iteration 29, loss = 0.12832526\n",
            "Iteration 30, loss = 0.12830188\n",
            "Iteration 31, loss = 0.12801257\n",
            "Iteration 32, loss = 0.12786492\n",
            "Iteration 33, loss = 0.12771712\n",
            "Iteration 34, loss = 0.12781547\n",
            "Iteration 35, loss = 0.12756548\n",
            "Iteration 36, loss = 0.12726457\n",
            "Iteration 37, loss = 0.12719728\n",
            "Iteration 38, loss = 0.12701822\n",
            "Iteration 39, loss = 0.12681687\n",
            "Iteration 40, loss = 0.12682281\n",
            "Iteration 41, loss = 0.12666762\n",
            "Iteration 42, loss = 0.12669404\n",
            "Iteration 43, loss = 0.12639070\n",
            "Iteration 44, loss = 0.12641778\n",
            "Iteration 45, loss = 0.12620686\n",
            "Iteration 46, loss = 0.12602744\n",
            "Iteration 47, loss = 0.12598284\n",
            "Iteration 48, loss = 0.12602492\n",
            "Iteration 49, loss = 0.13361087\n",
            "Iteration 50, loss = 0.12557912\n",
            "Iteration 51, loss = 0.12561490\n",
            "Iteration 52, loss = 0.12530164\n",
            "Iteration 53, loss = 0.12524801\n",
            "Iteration 54, loss = 0.12506002\n",
            "Iteration 55, loss = 0.12493210\n",
            "Iteration 56, loss = 0.12471425\n",
            "Iteration 57, loss = 0.12466959\n",
            "Iteration 58, loss = 0.12447998\n",
            "Iteration 59, loss = 0.12434969\n",
            "Iteration 60, loss = 0.12419109\n",
            "Iteration 61, loss = 0.12414348\n",
            "Iteration 62, loss = 0.12392702\n",
            "Iteration 63, loss = 0.12735363\n",
            "Iteration 64, loss = 0.12381133\n",
            "Iteration 65, loss = 0.12377290\n",
            "Iteration 66, loss = 0.12358978\n",
            "Iteration 67, loss = 0.12345955\n",
            "Iteration 68, loss = 0.12330921\n",
            "Iteration 69, loss = 0.12315684\n",
            "Iteration 70, loss = 0.12307788\n",
            "Iteration 71, loss = 0.12303219\n",
            "Iteration 72, loss = 0.12289251\n",
            "Iteration 73, loss = 0.12273826\n",
            "Iteration 74, loss = 0.12261757\n",
            "Iteration 75, loss = 0.12247714\n",
            "Iteration 76, loss = 0.12243837\n",
            "Iteration 77, loss = 0.12217975\n",
            "Iteration 78, loss = 0.12209512\n",
            "Iteration 79, loss = 0.12204480\n",
            "Iteration 80, loss = 0.12196880\n",
            "Iteration 81, loss = 0.12179639\n",
            "Iteration 82, loss = 0.12165424\n",
            "Iteration 83, loss = 0.12157260\n",
            "Iteration 84, loss = 0.12144468\n",
            "Iteration 85, loss = 0.12137596\n",
            "Iteration 86, loss = 0.12123177\n",
            "Iteration 87, loss = 0.12110212\n",
            "Iteration 88, loss = 0.12114615\n",
            "Iteration 89, loss = 0.12090584\n",
            "Iteration 90, loss = 0.12076588\n",
            "Iteration 91, loss = 0.12086758\n",
            "Iteration 92, loss = 0.12061214\n",
            "Iteration 93, loss = 0.12044751\n",
            "Iteration 94, loss = 0.12034742\n",
            "Iteration 95, loss = 0.12031449\n",
            "Iteration 96, loss = 0.12015883\n",
            "Iteration 97, loss = 0.12026298\n",
            "Iteration 98, loss = 0.11993093\n",
            "Iteration 99, loss = 0.11982381\n",
            "Iteration 100, loss = 0.11972979\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Anna-Sophia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(alpha=0.05, hidden_layer_sizes=(20, 50, 70, 100, 70, 50, 20),\n",
              "              learning_rate='adaptive', max_iter=100, solver='sgd',\n",
              "              verbose=True)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_y = train_y.astype('int')\n",
        "MLP.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dd721c2",
      "metadata": {
        "id": "6dd721c2"
      },
      "outputs": [],
      "source": [
        "y_pred = MLP.predict(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2639522e",
      "metadata": {
        "id": "2639522e",
        "outputId": "bc3aec77-43e8-4e1f-cc49-1583aea3398f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98     27432\n",
            "           1       0.76      1.00      0.86      3773\n",
            "\n",
            "    accuracy                           0.96     31205\n",
            "   macro avg       0.88      0.98      0.92     31205\n",
            "weighted avg       0.97      0.96      0.96     31205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_y, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7213e05e",
      "metadata": {
        "id": "7213e05e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
>>>>>>> c14fbab1a8cada26dbe9c99f4f651ebf2f02be2a
